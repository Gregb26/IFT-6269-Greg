{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKIz_ttKiG_1"
   },
   "source": [
    "# IFT6269 - Homework 4 - Hidden Markov Models\n",
    "**Due:**  Thursday, November 28, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whO8SD53Y9Vl"
   },
   "source": [
    "#### Name: GrÃ©goire Barrette    \n",
    "#### Student ID: 20175180\n",
    "#### Collaborators: None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02fg3bxiZOMv"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The file `EMGaussian.train` contains samples of data $\\{x_t\\}_{t=1}^T$ where $x_t \\in \\mathbb{R}^2$, with one datapoint per row. `EMGaussian.test` is structured similarly. This is the same data we used in Homework 3, but this time we use an HMM model to account for the possible temporal structure of the data. This means that we now consider each row of the dataset to be a point $x_t \\in \\mathbb{R}^2$ corresponding to some temporal process, rather than thinking of them as *independent* samples as we did in the last homework.\n",
    "\n",
    "We consider the following HMM model: the chain $(z_t)_{t=1}^T$ has  $K=4$ possible states, with an initial probability distribution $\\pi\\in\\Delta_4$ and a probability transition matrix  $A \\in \\mathbb{R}^{4 \\times 4}$ where $A_{ij} = p(z_t=i | z_{t-1} = j),$ and conditionally on the current state $z_t$, we have observations obtained from Gaussian emission probabilities $x_t| (z_t=k) \\sim \\mathcal{N}(x_t | \\mu_k, \\Sigma_k)$.  This is thus a generalization of a GMM since we now allow for time dependencie across the latent states $z_t$.\n",
    "\n",
    "This exercise has several implementation objectives:\n",
    "* **Sum-product**: probabilistic inference on the HMM\n",
    "* **Expectation-Maximization**: parameter estimation\n",
    "* **Viterbi**: decoding.\n",
    "\n",
    "**Note:** You may use the (*possibly corrected*) code you created for the previous assignment. Furthermore, notice there are some math questions in this notebook: do not forget to solve them!\n",
    "\n",
    "### Tasks\n",
    "0.   Get your own copy of this file via \"File > Save a copy in Drive...\",\n",
    "1.   Fill your personal information and collaborators at the top of this assignment, and rename the notebook accordingly, e.g., `hw4_thomasBayes.ipynb`\n",
    "2.   Read the instructions provided on each section and cell carefully,\n",
    "3.   Complete the exercises in the sections **Sum-product**, **Expectation-Maximization**, **Viterbi**, **Comparing methods** and **What about K?**.\n",
    "4.   Share your notebook with `ift6269.f23@gmail.com` (\"Share\" button on the top-right corner);\n",
    "5.   Share the link of your notebook on Gradescope (\"Share > Copy Link\", and paste the link in \"Homework 4 (Programming & Checklist)\" on Gradescope).\n",
    "    \n",
    "**Important**: You are allowed to collaborate with other students in both the math and coding parts of this assignment. However, the answers provided here must reflect your individual work. For that reason, you are not allowed to share this notebook, except for your submission to the TA for grading. **Don't forget to pin and save the version of the notebook you want to be graded on!**\n",
    "\n",
    "**Important**: Your are allowed to use functions **only** from the libraries imported here (`numpy` and `matplotlib`). In particular (but not limited to), you are **not allowed** to import functions from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3gk4JHNY9yW"
   },
   "outputs": [],
   "source": [
    "!wget http://www.iro.umontreal.ca/~slacoste/teaching/ift6269/A20/notes/hwk3data.zip\n",
    "!unzip hwk3data.zip\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "X_train = np.loadtxt(\"/content/hwk3data/EMGaussian.train\")\n",
    "X_test = np.loadtxt(\"/content/hwk3data/EMGaussian.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBJGYsmug-ZD"
   },
   "source": [
    "## Playground\n",
    "\n",
    "You are allowed to add as many cells and functions as you wish in this section, but not allowed to change the signature (name and inputs) of the functions we provided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2sgnsCmhG9p"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "#                       Code for plotting the results\n",
    "#                      ! DO NOT MODIFY THESE FUNCTIONS !\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def plot_smoothing(gamma, K=4, time_limit=100):\n",
    "    plt.figure(figsize=(14, 2*K))\n",
    "    plt.suptitle('Smoothing probabilities $p(z_t|x_1, ..., x_T)$', fontsize=16)\n",
    "    for k in range(K):\n",
    "        plt.subplot(K, 1, 1+k)\n",
    "        plt.plot(range(1, time_limit+1), gamma[:time_limit, k] )\n",
    "        plt.ylabel(r'$p(z_t = ' + str(k+1) + ' | x_{1:T})$')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True)\n",
    "    plt.xlabel('t')\n",
    "    plt.show()\n",
    "\n",
    "def plot_labelling(X, labels, mus, title=\"\"):\n",
    "    shapes = ['o', '*', 'v', '+']\n",
    "    colors = [[31, 119, 180], [255, 127, 14], [44, 160, 44], [148, 103, 189],\n",
    "              [140, 86, 75], [227, 119, 194], [127, 127, 127], [188, 189, 34]]\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    cs = [colors[int(_) % len(colors)] for _ in labels]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=np.array(cs)/255.)\n",
    "    plt.scatter(mus[:, 0], mus[:, 1], marker='o', c='#d62728')\n",
    "    plt.xlim(-12, 12), plt.ylim(-12, 12)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_dominoes(data):\n",
    "    # Pick max from data per timestep\n",
    "    data_maxhot = (data == data.max(axis=1, keepdims=True))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 3)\n",
    "    ax.pcolor(1 - data_maxhot[:100,::-1].T, cmap=plt.cm.gray, alpha=0.6)\n",
    "    ax.set_yticks(np.arange(4) + 0.5, minor=False)\n",
    "    ax.set_yticklabels([4,3,2,1], minor=False)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYffvc08munF"
   },
   "source": [
    "## Sum-product [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFM6NpRKYvKr"
   },
   "source": [
    "### $\\alpha$, $\\beta$ recursions\n",
    "\n",
    "Implement the $\\alpha$ and $\\beta$-recursions seen in class (and that can be found in chapter 12 of Mike's book with slightly different notation). Recall that $\\alpha(z_t) := p(z_t, x_{1:t})$, $\\beta(z_t) := p(x_{(t+1):T} | z_t)$. Implement also a function to compute the emission probabilties $\\epsilon_k(x_t) := p(x_t|z_t=k) = \\mathcal{N}(x_t|\\mu_k, \\Sigma_k)$.\n",
    "\n",
    "For numerical stability reasons, you are expected to implement your algorithms using **log probabilities** unless noted explicitly!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4njzQRaJa5f3"
   },
   "outputs": [],
   "source": [
    "def log_emission(X, mus, sigmas) :\n",
    "    \"\"\"\n",
    "    (Log) probabilities under a Gaussian emission model for a time-homogeneous HMM\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of observations\n",
    "        mus: [Kx2] matrix of latent-conditional emission means\n",
    "        sigmas: [Kx2x2] tensor of latent-conditional emission covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        log_eps: [TxK] matrix of log emission probabilities: log p(x_t | z_t = k)\n",
    "    \"\"\"\n",
    "    T, K = X.shape[0], mus.shape[0]\n",
    "\n",
    "    # TODO\n",
    "    log_eps = np.zeros((T, K))\n",
    "\n",
    "    return log_eps\n",
    "\n",
    "def log_alpha_recursion(X, A, log_eps, pi) :\n",
    "    \"\"\"\n",
    "    (Log) alpha recursion for a time-homogeneous HMM with Gaussian emissions\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of observations\n",
    "        A: [KxK] transition matrix\n",
    "        log_eps: [TxK] matrix of log emission probabilities: log p(x_t | z_t = k)\n",
    "        pi: [Kx1] initial latent state distribution\n",
    "\n",
    "    Returns:\n",
    "        log_alpha: [TxK] vector containing log p(z_t , x_{1:t})\n",
    "    \"\"\"\n",
    "    T, K = log_eps.shape\n",
    "\n",
    "    # TODO\n",
    "    log_alpha = np.zeros((T, K))\n",
    "\n",
    "    return log_alpha\n",
    "\n",
    "def log_beta_recursion(X, A, log_eps) :\n",
    "    \"\"\"\n",
    "    (Log) beta recursion for a time-homogeneous HMM with Gaussian emissions\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of observations\n",
    "        A: [KxK] transition matrix\n",
    "        log_eps: [TxK] matrix of log emission probabilities: log p(x_t | z_t = k)\n",
    "\n",
    "    Returns:\n",
    "        log_beta: [TxK] vector containing log p(x_{t+1:T} | z_t)\n",
    "    \"\"\"\n",
    "    T, K = log_eps.shape\n",
    "\n",
    "    # TODO\n",
    "    log_beta = np.zeros((T,K))\n",
    "\n",
    "    return log_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j9ZGUaHhWR7"
   },
   "source": [
    "Using the two functions above, implement the computation of the *smoothing* distribution $p(z_t|x_1,\\dots,x_T)$ and pair-marginals $p(z_t,z_{t+1}|x_1,\\dots,x_T)$. Here use log probabilities from the $\\alpha$ and $\\beta$, but return a normal (not log!) probability, i.e, a number in $[0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-OzX3lPhVnd"
   },
   "outputs": [],
   "source": [
    "def smoothing(log_alpha, log_beta):\n",
    "    \"\"\"\n",
    "    Smoothing probabilities for a time-homogeneous HMM with Gaussian emissions\n",
    "\n",
    "    Inputs:\n",
    "        log_alpha: [TxK] matrix containing log p(z_t , x_{1:t})\n",
    "        log_beta: [TxK] matrix containing log p(x_{t+1:T} | z_t)\n",
    "\n",
    "    Returns:\n",
    "        gamma: [TxK] matrix of smoothing probabilities p(z_t | x_{1:T})\n",
    "    \"\"\"\n",
    "    T, K = log_alpha.shape\n",
    "\n",
    "    # TODO\n",
    "    gamma = np.zeros((T, K))\n",
    "\n",
    "    return gamma\n",
    "\n",
    "def pair_marginals(log_alpha, log_beta, A, log_eps):\n",
    "    \"\"\"\n",
    "    Pair marginals for a time-homogeneous HMM with Gaussian emissions\n",
    "\n",
    "    Inputs:\n",
    "        log_alpha: [TxK] matrix containing log p(z_t , x_{1:t})\n",
    "        log_beta: [TxK] matrix containing log p(x_{t+1:T} | z_t)\n",
    "        A: [KxK] transition matrix\n",
    "        log_eps: [TxK] matrix of log emission probabilities: log p(x_t | z_t = k)\n",
    "\n",
    "    Returns:\n",
    "        psi: [TxKxK] numpy tensor of pair marginal probabilities p(z_t, z_{t+1} | x_{1:T})\n",
    "    \"\"\"\n",
    "    T, K = log_alpha.shape\n",
    "\n",
    "    # TODO\n",
    "    psi = np.zeros((T, K, K))\n",
    "\n",
    "    # Just as above, we keep psi of length T on the first dimension\n",
    "    psi[T-1, ...] = np.zeros((K,K))\n",
    "\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs2UsTT9luLm"
   },
   "source": [
    "### Inference with ad hoc parameters\n",
    "\n",
    "Consider using the same parameters for the means and covariance matrix of the 4 Gaussians that you should have learned in Homework 3 for EM with general covariance matrices. For convenience, we give to you below:\n",
    "\\begin{align*}\n",
    "\\mu_1 &= \\left( \\begin{aligned}\n",
    "-2&.0344 \\\\\n",
    "4&.1726\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\mu_2 &= \\left( \\begin{aligned}\n",
    "3&.9779 \\\\\n",
    "3&.7735\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\mu_3 &= \\left( \\begin{aligned}\n",
    "3&.8007 \\\\\n",
    "-3&.7972\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\mu_4 &= \\left( \\begin{aligned}\n",
    "-3&.0620 \\\\\n",
    "-3&.5345\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\Sigma_1 &= \\left( \\begin{aligned}\n",
    "2&.9044 &   0&.2066 \\\\\n",
    "0&.2066 &   2&.7562\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\Sigma_2 &= \\left( \\begin{aligned}\n",
    "0&.2104 &   0&.2904 \\\\\n",
    "0&.2904 &   12&.2392\n",
    "\\end{aligned}\n",
    "\\right)\n",
    "\\\\\n",
    "\\\\\n",
    "\\Sigma_3 &= \\left( \\begin{aligned}\n",
    "0&.9213 &   0&.0574 \\\\\n",
    "0&.0574 &   1&.8660\n",
    "\\end{aligned}\n",
    "\\right) &\n",
    "\\Sigma_4 &= \\left( \\begin{aligned}\n",
    "6&.2414 &   6&.0502 \\\\\n",
    "6&.0502 &   6&.1825\n",
    "\\end{aligned}\n",
    "\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "Consider a uniform initial probability distribution $\\pi_k = \\frac{1}{4}$, and set $A$ to be the matrix with diagonal coefficients $A_{ii}=\\frac{1}{2}$ and off-diagonal coefficients $A_{ij}=\\frac{1}{6}$ for all $(i,j) \\in \\{1,\\ldots,4\\}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtpV5qugl0QY"
   },
   "outputs": [],
   "source": [
    "# We have already typed in all these wonderful numbers for you ;)\n",
    "K = 4\n",
    "\n",
    "pi = np.array(4 * [1/4])\n",
    "\n",
    "A = (1/2) * np.eye(4) + (1/6) * (np.ones(4) - np.eye(4))\n",
    "\n",
    "mus = np.array([[-2.0344, 4.1726], \\\n",
    "                      [3.9779 , 3.7735], \\\n",
    "                      [3.8007 ,-3.7972], \\\n",
    "                      [-3.0620,-3.5345]])\n",
    "\n",
    "sigmas = np.array([[[2.9044, 0.2066],[0.2066, 2.7562 ]],  \\\n",
    "                   [[0.2104, 0.2904],[0.2904, 12.2392]],  \\\n",
    "                   [[0.9213, 0.0574],[0.0574, 1.8660 ]],  \\\n",
    "                   [[6.2414, 6.0502],[6.0502, 6.1825 ]]])\n",
    "\n",
    "# We store this for EM later\n",
    "INIT_PARAMS = (K, pi, A, mus, sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQtJUjviqMLp"
   },
   "source": [
    "Let $\\{x_1,\\dots,x_T\\}$ be the **test** data. Using your functions above, for all $t$ on the test data, compute $\\log(\\alpha_t)$, $\\log(\\beta_t)$ and  $p(z_t | x_1,\\dots,x_T)$. Finally, plot $p(z_t|x_1,\\dots,x_T)$ for each of the 4 states as a function of $t$ for the first 100 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86HFKjerjj7-"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Compute emission (log) probabilities\n",
    "log_eps = log_emission(X_test, mus, sigmas)\n",
    "assert log_eps.shape == (X_test.shape[0], K), \"Erroneous shape of `log_eps`\"\n",
    "assert np.all(log_eps <= 0), \"Erroneous domain for log p(x_t|z_t)\"\n",
    "\n",
    "# Perform (log) alpha and beta recursions\n",
    "log_alpha = log_alpha_recursion(X_test, A, log_eps, pi)\n",
    "log_beta = log_beta_recursion(X_test, A, log_eps)\n",
    "\n",
    "assert np.allclose(log_alpha[0], [-19.91, -197.75, -42.54, -3.85], rtol=1e-1), \"Erroneous initial condition for alpha recursion\"\n",
    "assert np.allclose(log_beta[-1], 0), \"Erroneous boundary condition for beta recursion\"\n",
    "\n",
    "# Compute smoothing probabilities\n",
    "gamma = smoothing(log_alpha, log_beta)\n",
    "assert np.allclose(gamma.sum(axis=1), 1), \"`gamma` is not a probability distribution\"\n",
    "\n",
    "# Now we do the plot of the smoothing probability for each of the 4 states\n",
    "plot_smoothing(gamma)\n",
    "\n",
    "# Edge maringals, for the assert below\n",
    "psi = pair_marginals(log_alpha, log_beta, A, log_eps)\n",
    "assert np.allclose(psi.sum(axis=(1,2)), 1), \"`psi` must be a distribution over z_t, z_{t+1} pairs for every x_t\"\n",
    "assert np.allclose(gamma[1:], psi.sum(axis=(2))), \"`gamma` should correspond to the marginal of `psi` over z_{t+1}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcgUP-TuwDEh"
   },
   "source": [
    "## Expectation-Maximization [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0ZoK5joZsOD"
   },
   "source": [
    "### M-step updates\n",
    "Derive the M-step update for $\\hat{\\pi}$, $\\hat{A}$, $\\hat{\\mu}_k$ and $\\hat{\\Sigma}_k$ (for $k=1,\\ldots, 4)$ during the EM algorithm, as a function of the quantities computed during the E step. Note that for the estimate of $\\pi$, we only have *one* long chain here!\n",
    "\n",
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP7Yx_6tv721"
   },
   "source": [
    "### EM implementation\n",
    "\n",
    "Implement the EM algorithm to learn the parameters of the model ($\\pi,A,\\mu_k,\\Sigma_k,\\: k=1\\ldots,4$). Use the parameters from question 2 for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOK_bx_rwdw8"
   },
   "outputs": [],
   "source": [
    "def avg_llike(X, pi, A, mus, sigmas):\n",
    "    \"\"\"\n",
    "    Computes the average log-likelihood of parameters pi, A, mus, sigmas on the\n",
    "    data X.\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of data observations\n",
    "        pi: [K] estimated initial latent distribution\n",
    "        A: [KxK] estimated transition matrix\n",
    "        mus: [Kx2] matrix of estimated emission means\n",
    "        sigmas: [Kx2x2] tensor of estimated emission covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        avg_llike: scalar average log-likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return 0\n",
    "\n",
    "def Estep(X, pi, A, mus, sigmas):\n",
    "    \"\"\"\n",
    "    E step of the EM algorithm\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of data observations\n",
    "        pi: [K] estimated initial latent distribution\n",
    "        A: [KxK] estimated transition matrix\n",
    "        mus: [Kx2] matrix of estimated emission means\n",
    "        sigmas: [Kx2x2] tensor of estimated emission covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        gamma: [TxK] matrix of smoothing probabilities p(z_t | x_{1:T})\n",
    "        psi: [TxKxK] numpy tensor of pair marginal probabilities p(z_t, z_{t+1} | x_{1:T})\n",
    "    \"\"\"\n",
    "\n",
    "    T = z.shape[0]\n",
    "    K = pi.shape\n",
    "\n",
    "    # TODO\n",
    "    gamma = np.zeros(T, K)\n",
    "    psi = np.zeros(T, K, K)\n",
    "\n",
    "    return gamma, psi\n",
    "\n",
    "def Mstep(X, gamma, psi):\n",
    "    \"\"\"\n",
    "    M step of the EM algorithm\n",
    "\n",
    "    Inputs:\n",
    "        gamma: [TxK] matrix of smoothing probabilities p(z_t | x_{1:T})\n",
    "        psi: [TxKxK] numpy tensor of pair marginal probabilities p(z_t, z_{t+1} | x_{1:T})\n",
    "\n",
    "    Returns:\n",
    "        pi: [K] estimated initial latent distribution\n",
    "        A: [KxK] estimated transition matrix\n",
    "        mus: [Kx2] matrix of estimated emission means\n",
    "        sigmas: [Kx2x2] tensor of estimated emission covariance matrices\n",
    "    \"\"\"\n",
    "\n",
    "    K = gamma.shape[-1]\n",
    "\n",
    "    # TODO\n",
    "    pi = np.zeros(K)\n",
    "    A = np.zeros(K,K)\n",
    "    mus = np.zeros(K, 2)\n",
    "    sigmas = np.zeroz(K, K, 2)\n",
    "\n",
    "    return pi, A, mus, sigmas\n",
    "\n",
    "\n",
    "def exp_max(X_tr, X_ts, init_params):\n",
    "    \"\"\"\n",
    "    Estimates the parameters of an HMM using training data X via the EM algorithm\n",
    "\n",
    "    Inputs:\n",
    "        X_tr: [T_trainx2] matrix of training observations\n",
    "        X_tr: [T_testx2] matrix of test observations\n",
    "        init_params: tuple of initialization parameters from previous question\n",
    "\n",
    "    Returns:\n",
    "        pi: [K] estimated initial latent distribution\n",
    "        A: [KxK] estimated transition matrix\n",
    "        mus: [Kx2] matrix of estimated emission means\n",
    "        sigmas: [Kx2x2] tensor of estimated emission covariance matrices\n",
    "        train_avg_llike: list containing the average training log likelihood on each iteration\n",
    "        test_avg_llike: list containing the average test log likelihood on each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # Set initialization from parameters given in previous question\n",
    "    (K, pi, A, mus, sigmas) = init_params\n",
    "\n",
    "    # TODO\n",
    "    train_avg_llike  = []\n",
    "    test_avg_llike  = []\n",
    "\n",
    "    return pi, A, mus, sigmas, train_avg_llike, test_avg_llike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXWievd8v8RG"
   },
   "source": [
    "Learn the parameters of the HMM using the **training** data and plot the average log-likelihood on the train and test data as a function of the iterations of the algorithm. Comment on your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwaLZpcSw3DZ"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Estimate parameters using EM and collect results\n",
    "res = exp_max(X_train, X_test, INIT_PARAMS)\n",
    "pi_em, A_em, mus_em, sigmas_em, train_avg_llike, test_avg_llike = res\n",
    "\n",
    "# Checks\n",
    "assert np.all(np.diff(train_avg_llike) >= 0), \"The training log-likelihoods of the EM algorithm must be non-decreasing.\"\n",
    "\n",
    "# Plot the log-likelihoods\n",
    "plt.plot(train_avg_llike, label='Train')\n",
    "plt.plot(test_avg_llike, label='Test')\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.title('Average Log-likelihood', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K52wGxCw5tN"
   },
   "source": [
    "Complete the following table with the values of the average log-likelihood for the Gaussian mixture models (with a) scaled identities and b) full covariances) and of the HMM on the train and on the test data.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Method | Train Avg. Log-Like | Test Avg. Log-Like\n",
    "--- | --- | ---\n",
    "*Scaled Identities GMM* | ??? | ???\n",
    "*Full Covariance GMM* | ??? | ???\n",
    "*Hidden Markov Model* | ??? | ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcASaxLKyeyg"
   },
   "source": [
    "Does it make sense to make this comparison? Why? If so, what conclusions can you draw?. Moreover, briefly compare these log-likelihoods those obtained for the different models in the previous homework.\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPgWURgkZuhf"
   },
   "source": [
    "## Viterbi [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DV0nH0nI4nLr"
   },
   "source": [
    "### Pseudocode\n",
    "\n",
    "Provide a description and pseudo-code for the Viterbi decoding algorithm (a.k.a. MAP inference algorithm or max-product algorithm) that estimates the most likely sequence of states: $\\arg \\max_z p(z_1,\\dots,z_T | x_1,\\dots,x_T)$.\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKXMdRvw5D15"
   },
   "source": [
    "### Viterbi implementation\n",
    "\n",
    "Implement Viterbi decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOv-BCCr4mpZ"
   },
   "outputs": [],
   "source": [
    "def viterbi(X, pi, A, mus, sigmas):\n",
    "    \"\"\"\n",
    "    Viterbi decoding for a time-homogeneous HMM with Gaussian emissions\n",
    "\n",
    "    Inputs:\n",
    "        X: [Tx2] matrix of observations\n",
    "        pi: [Kx1] initial latent state distribution\n",
    "        A: [KxK] transition matrix\n",
    "        mus: [Kx2] matrix of latent-conditional emission means\n",
    "        sigmas: [Kx2x2] tensor of latent-conditional emission covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        z: [TxK] one-hot encoding of most probable state sequence z_{1:T} given x_{1:T}\n",
    "    \"\"\"\n",
    "\n",
    "    T, K = X.shape[0], mus.shape[0]\n",
    "\n",
    "    # TODO\n",
    "    z = np.zeros((T, K))\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jaapR-F5TLj"
   },
   "source": [
    "Using the set of parameters learned with the EM algorithm on the **training** set, compute the most likely sequence of states for the **training** data with the Viterbi algorithm. Plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6pOjgUjYunV"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Get one_hot_labels from Viterbi and turn them into 0, ..., K-1 \"numeric\" labels\n",
    "one_hot_labels = viterbi(X_train, pi_em, A_em, mus_em, sigmas_em)\n",
    "viterbi_labels =  np.argmax(one_hot_labels, axis=1)\n",
    "\n",
    "# Plot the results\n",
    "plot_labelling(X_train, viterbi_labels, mus, 'Viterbi Labelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfcE-T58AMBY"
   },
   "source": [
    "## Comparing methods [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTvixra6BJQn"
   },
   "source": [
    "Use the parameters learned using EM on the **training** set to compute the smoothing marginal probability $p(z_t|x_1,\\dots,x_T)$ for each datapoint in the **test** set to be in state $\\{1,2,3,4\\}$. Plot the probability of being in that state as a function of time for the 100 first points, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs_Na5LVBncw"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Compute emission (log) probabilities\n",
    "log_eps = log_emission(X_test, mus_em, sigmas_em)\n",
    "\n",
    "# Perform (log) alpha and beta recursions\n",
    "log_alpha = log_alpha_recursion(X_test, A_em, log_eps, pi_em)\n",
    "log_beta = log_beta_recursion(X_test, A_em, log_eps)\n",
    "\n",
    "# Compute smoothing probabilities\n",
    "gamma = smoothing(log_alpha, log_beta)\n",
    "\n",
    "# Now we do the plot of the smoothing probability for each of the 4 states\n",
    "plot_smoothing(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPxqZ7-2Bw9K"
   },
   "source": [
    "For each of these same 100 points, compute their most likely state according to the marginal probability computed in the previous question. Make a plot representing the most likely state in $\\{1,2,3,4\\}$ as function of time for these 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSDCC98pB6vS"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "plot_dominoes(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k-uV4z1B7JX"
   },
   "source": [
    "Run Viterbi on the **test** data. Make a similar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8eR-QX4KCIQu"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "#     Do NOT change the contents of this cell. This is just for execution.     #\n",
    "#   If you respected function signatures, this cell should run without changes #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Get one_hot_labels from Viterbi and turn them into 0, ..., K-1 \"numeric\" labels\n",
    "one_hot_labels = viterbi(X_test, pi_em, A_em, mus_em, sigmas_em)\n",
    "\n",
    "# Plot the results\n",
    "plot_dominoes(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWmyFwcgCKes"
   },
   "source": [
    "Compare the most likely sequence of states obtained for the 100 first data points with the sequence of states obtained in the previous question. Comment on your observations.\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmq_SzN1AhN3"
   },
   "source": [
    "## What about $K$? [5 pts]\n",
    "\n",
    "In this problem the number of states $K$ was known. How should one choose the number of states if it is unknown a priori?\n",
    "\n",
    "**Answer:**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
