{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8p_rVKVWH2w"
      },
      "source": [
        "# IFT6269 - Homework 2 - Linear Classification\n",
        "\n",
        "**Due**: Thursday, October 17, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erSG6CCYGYE6"
      },
      "source": [
        "#### Name: Grégoire Barrette\n",
        "#### Student ID: 20175180\n",
        "#### Collaborators: None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SloWv9XFBdxL"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "In this assignment you will compare the performance of several algorithm on some synthetic classification tasks. The assignment consists of two parts: 1) a derivation of the estimators for the Fisher LDA with general covariance matrix derivation and 2) the implementation of several classification algorithms.\n",
        "\n",
        "### Tasks\n",
        "0.   Get your own copy of this file via \"File > Save a copy in Drive...\",\n",
        "1.   Fill your personal information and collaborators at the top of this assignment, and rename the notebook accordingly, e.g., `hw2_thomasBayes.ipynb`\n",
        "2.   Read the instructions provided on each section and cell carefully,\n",
        "3.   Answer the section **Math Questions on Fisher LDA**,\n",
        "4.   Implement the requested algorithms in section **Playground** following the example provided in dummy_model`,\n",
        "5.   In section **Model Comparison**, for each of the datasets:\n",
        "    *   use the training data to estimate the parameters of each of the 4 algorithms,\n",
        "    *   plot the corresponding estimated decision boundary and the training set using the function `plot_results`,\n",
        "    *   compute the training and test accuracy and fill the table in each section by hand,\n",
        "    *   compare the performance of each of the algorithms and provide an explanation for your observations based on, for example, their accuracy, overfitting/generalization properties, whether the assumptions of each algorithms are satisfied by the data, etc.\n",
        "    \n",
        "**Important**: You are allowed to collaborate with other students in both the math and coding parts of this assignment. However, the answers provided here must reflect your individual work. For that reason, you are not allowed to share this notebook, except for your submission to the TA for grading. **Don't forget to pin and save the version of the notebook you want to be graded on!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJlUxcYmBYCK",
        "outputId": "145d0c61-8e34-40e8-dcfe-54eab1cb03e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-v0_8-white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvqU0cT7WGKw"
      },
      "source": [
        "You get 3 different datasets (**A**, **B**, **C**) with a training and test set for each, i.e., 6 sets in total. Each row represents a sample of data $(x_i, y_i)$. There are three columns: the first two give the coordinates for $x_i \\in \\mathbb{R}^2$; the third column gives the class label $y_i \\in \\{0, 1\\}$. The datasets are all generated from some kind of mixture of Gaussians generative model. The train and test sets are generated from the same distribution for each types of dataset.\n",
        "\n",
        "To help your interpretation, we give you the actual\n",
        "generating process. However, keep in mind that normally we would not know the information about the generating process. In this\n",
        "assignment, we will compare different classification approaches.\n",
        "*  **Dataset A**: the class-conditionals for this dataset are Gaussians with different means, but with a shared covariance matrix $\\Sigma$.\n",
        "*  **Dataset B**: similar generating process but the covariance matrices are different for the two classes.\n",
        "*  **Dataset C**: here one class is a mixture of two Gaussians, while the other class is a single Gaussian (with no sharing).\n",
        "\n",
        "Test the different models learnt from the corresponding training data on these test data. Compute for each model the accuracy (i.e.  the fraction of the data correctly classified) on the training set and compute it as well for the test set. And compare the performances of the different methods on the three datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bd37lDIfFx74"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d_ix \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t_flag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 5\u001b[0m         data[t_flag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m d_ix] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhwk2data/classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39md_ix\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mt_flag)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "data = {}\n",
        "\n",
        "for d_ix in [\"A\", \"B\", \"C\"]:\n",
        "    for t_flag in [\"train\", \"test\"]:\n",
        "        data[t_flag + \"_\" + d_ix] = np.loadtxt(\"hwk2data/classification\"+d_ix+\".\"+t_flag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8zV4Tcip4qw"
      },
      "source": [
        "### Models\n",
        "\n",
        "**Fisher LDA**\n",
        ">  We first consider the Fisher LDA model as seen in class: given the class variable, the data are assumed to be Gaussians with different means for different classes but with the same covariance matrix: $Y \\sim \\text{Bernoulli}(\\pi)$, $X | \\{Y = j\\} \\sim \\mathcal{N}(\\mu_j, \\Sigma)$.\n",
        "\n",
        "**Logistic Regression**\n",
        ">  Implement logistic regression for an affine function $f(x) = w^Tx+b$ (do not forget the constant term – you can use the bias feature trick) using the IRLS algorithm (Newton’s method) which was described in class. Hint: never compute the matrix inverse by itself – this is not numerically stable when the Hessian might become ill-conditioned.\n",
        "\n",
        "**Linear regression**\n",
        "\n",
        "> As mentioned in class, we can forget that the class $y$ can only take the two values $0$ or $1$ and think of it as a real-valued variable on which we can do standard linear regression (least-squares). Here, the Gaussian noise model on $y$ does not make any sense from a generative point of view; but we can still do least-squares to estimate the parameters of a linear decision boundary (you’ll be surprised by its performance despite coming from a “bad”\n",
        "generative model!). Implement linear regression (for an affine function $f(x) = w^Tx + b$) by solving the normal equations on each dataset (with no regularization).\n",
        "\n",
        "\n",
        "**QDA**\n",
        ">  We finally relax the assumption that the covariance matrices for the two classes are the same. So, given the class label, the data are now assumed to be Gaussian with means and covariance matrices which are a priori different:\n",
        "$Y \\sim \\text{Bernoulli}(\\pi)$, $X | \\{Y = j\\} \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIWLS_7ywDxx"
      },
      "source": [
        "## Math Questions on Fisher LDA\n",
        "\n",
        "This week's math questions will be typed directly in this notebook, i.e., no scanned hand-written answers accepted! You can use standard $\\LaTeX$ syntax here!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQbv3qReyB5Y"
      },
      "source": [
        "**Question A:** Derive the form of the maximum likelihood estimator for the Fisher LDA model described above. Hint: you can re-use some of the tricks presented in class for the MLE of a multivariate Gaussian, but adapted to this setting. You can get inspiration from Section 7.2 in Mike’s book (which covers the case where $\\Sigma$ is diagonal).\n",
        "\n",
        "**Answer:**\n",
        "The Fisher LDA model is has $p(y_n|\\pi) = \\text{Bernoulli}(\\pi) = \\pi^{y_n}(1-\\pi)^{1-y_n}$ and $p(x_n | y_n, \\theta) = \\mathcal{N}(\\mu_1,\\mu_0, \\Sigma) = \\mathcal{N}(\\mu_1,\\Sigma)^{y_n}\\mathcal{N}(\\mu_0,\\Sigma)^{1-y_n}$. The log-likelyhood is then\n",
        "\\begin{align}\n",
        "  \\ell(\\pi,\\mu_1,\\mu_0,\\Sigma) &= \\log p(x,y),\\\\\n",
        "  &= \\log \\prod_{n=1}^Np(y_n|\\pi)p(x_n|y_n,\\mu_1,\\mu_0,\\Sigma),\\\\\n",
        "  &= \\sum_{n=1}^N(\\log p(y_n|\\pi) + \\log p(x_n|y_n,\\mu_1,\\mu_0,\\Sigma))\\\\\n",
        "  &=\\sum_{n=1}^N\\{y_n\\log\\pi + (1-y_n)\\log(1-\\pi)+y_n(-\\log 2\\pi -\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x_n-\\mu_1)^T\\Sigma^{-1}(x_n-\\mu_1))+(1-y_n)(-\\log 2\\pi\\\\\n",
        "  & -\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x_n-\\mu_0)^T\\Sigma^{-1}(x_n-\\mu_0))\\}.\n",
        "\\end{align}\n",
        "\n",
        "Now, for the MLE of $\\pi$:\n",
        "$$\\hat{\\pi}_{ML} = \\text{argmax}_\\pi\\ell(\\pi,\\mu_1,\\mu_0,\\Sigma).$$\n",
        "That is, I need to take the gradient of the log-likelyhood with respect to $\\pi$ and set it to zero. Let me drop all the terms that do not depend on $\\pi$ (the $\\pi$'s coming from the Gaussians are constants, not the variable):\n",
        "\\begin{align}\n",
        "  \\nabla_\\pi \\ell(\\pi,\\mu_1,\\mu_0,\\Sigma) &= \\sum_{n=1}^N\\nabla_\\pi(y_n\\log\\pi+(1-y_n)\\log(1-\\pi))\\\\\n",
        "  &\\sum_{n=1}^N\\frac{y_n}{\\pi}-\\sum_{n=1}^N\\frac{1-y_n}{1-\\pi}=0\\\\\n",
        "  &\\sum_{n=1}^N\\frac{y_n}{\\pi} = \\sum_{n=1}^N\\frac{1-y_n}{1-\\pi}\\\\\n",
        "  &\\sum_{n=1}^Ny_n(1-\\pi) = \\sum_{n=1}^N\\pi(1-y_n)\\\\\n",
        "  &\\sum_{n=1}^Ny_n = \\sum_{n=1}^N\\pi = N\\pi\n",
        "\\end{align}\n",
        "So\n",
        "$$\\hat{\\pi}_{ML} = \\frac{\\sum_{n=1}^N y_n}{N}.$$\n",
        "The MLE of $\\mu_1$ is\n",
        "$$\\hat{\\mu}_{1ML} = \\text{argmax}_{\\mu_1}\\ell(\\pi,\\mu_1,\\mu_0,\\Sigma).$$\n",
        "Let me drop all the terms that do not depend on $\\mu_1$:\n",
        "\\begin{align}\n",
        "\\nabla_{\\mu_1} \\ell(\\pi,\\mu_1,\\mu_0,\\Sigma) &= \\sum_{n=1}^N\\nabla_{\\mu_1}(-\\frac{y_n}{2}(x_n-\\mu_1)^T\\Sigma^{-1}(x_n-\\mu_1)) = 0\\\\\n",
        "&\\sum_{n=1}^Ny_n(x_n-\\mu_1) = 0\\\\\n",
        "&\\sum_{n=1}^Ny_nx_n = \\sum_{n=1}^Ny_n\\mu_1.\n",
        "\\end{align}\n",
        "So\n",
        "$$\\hat{\\mu}_{1ML} = \\frac{\\sum_{n=1}^Ny_nx_n}{\\sum_{n=1}^Ny_n}.$$\n",
        "The MLE for $\\mu_0$ is very similar, one just has to replace $y_n$ by $1-y_n$:\n",
        "$$\\hat{\\mu}_{1ML} = \\frac{\\sum_{n=1}^N(1-y_n)x_n}{\\sum_{n=1}^N(1-y_n)}.$$\n",
        "Finally, the MLE for $\\Sigma$ is\n",
        "$$\\hat{\\Sigma}_{ML} = \\text{argmax}_{\\Sigma}\\ell(\\pi,\\mu_1,\\mu_0,\\Sigma).$$\n",
        "Again, taking a gradient with respect to $\\Sigma^{-1}$ and setting it equal to zero gives\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{\\Sigma^{-1}} \\ell(\\pi,\\mu_1,\\mu_0,\\Sigma) &= \\sum_{n=1}^N\\nabla_{\\Sigma^{-1}}(y_n(-\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x_n-\\mu_1)^T\\Sigma^{-1}(x_n-\\mu_1))+(1-y_n)(-\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x_n-\\mu_0)^T\\Sigma^{-1}(x_n-\\mu_0))) = 0\\\\\n",
        "&\\sum_{n=1}^N(y_n(-\\frac{\\Sigma}{2}-\\frac{1}{2}(x_n-\\mu_1)(x_n-\\mu_1)^T)+(1-y_n)(-\\frac{\\Sigma}{2}-\\frac{1}{2}(x_n-\\mu_0)(x_n-\\mu_0)^T)) = 0\\\\\n",
        "&\\sum_{n=1}^N\\{-\\frac{y_n}{2}(x_n-\\mu_1)(x_n-\\mu_1)^T-\\frac{\\Sigma}{2} -\\frac{1}{2}(1-y_n)(x_n-\\mu_0)(x_n-\\mu)^T\\} = 0\\\\\n",
        "&\\sum_{n=1}^N\\{y_n(x_n-\\mu_1)(x_n-\\mu_1)^T+(1-y_n)(x_n-\\mu_0)(x_n-\\mu)^T\\} = \\sum_{n=1}^N\\Sigma = N\\Sigma.\n",
        "\\end{align}\n",
        "Where I've used the facts that $\\nabla_{\\Sigma^{-1}}\\log|\\Sigma| = \\Sigma$ and $\\nabla_{\\Sigma^{-1}}(x_n-\\mu)^T\\Sigma^{-1}(x_n-\\mu) = (x_n-\\mu)(x_n-\\mu)^T$, both of which were proven in class. The MLE for $\\Sigma$ is then\n",
        "$$\\hat{\\Sigma}_{ML} = \\frac{1}{N}\\left\\{\\sum_{n=1}^Ny_n(x_n-\\hat{\\mu}_{1ML})(x_n-\\hat{\\mu}_{1ML})^T+\\sum_{n=1}^N(1-y_n)(x_n-\\hat{\\mu}_{0ML})(x_n-\\hat{\\mu}_{0ML})^T\\right\\}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvJTKvjVyUtb"
      },
      "source": [
        "**Question B:** What is the form of the conditional distribution $p(y = 1|x)$? Compare with the form of logistic regression\n",
        "\n",
        "**Answer:**\n",
        "From Baye's rule\n",
        "$$p(y=1 | x) = \\frac{p(x|y=1)p(y=1)}{p(x)}.$$\n",
        "\n",
        "I know everything in the LHS but $p(x)$. However, I know that it is the marginal\n",
        "$$p(x) = \\sum_yp(x,y),$$\n",
        "and the joint can be decomposed using the chain rule of probability\n",
        "$$p(x,y) = p(x|y)p(y).$$\n",
        "Hence,\n",
        "$$p(y=1|x) = \\frac{p(x|y=1)p(y=1)}{\\sum_{y=0}^1p(x|y)p(y)}=\\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}.$$\n",
        "\n",
        "Using the functional form of these quantities, I have\n",
        "\\begin{align}\n",
        "p(y=1|x) &= \\frac{\\mathcal{N}(\\mu_1,\\Sigma)\\pi}{\\mathcal{N}(\\mu_1,\\Sigma)\\pi+\\mathcal{N}(\\mu_0,\\Sigma)(1-\\pi)}\\\\\n",
        "&=\\frac{1}{1 +\\frac{\\mathcal{N}(\\mu_0,\\Sigma)(1-\\pi)}{\\mathcal{N}(\\mu_1,\\Sigma)\\pi}}\\\\\n",
        "&=\\frac{1}{1+e^{-f(x)}}\\\\\n",
        "&= \\sigma(f(x)),\n",
        "\\end{align}\n",
        "where $\\sigma(x)$ is the logistic function and I've defined\n",
        "$$f(x) \\equiv \\log \\frac{\\mathcal{N}(\\mu_1,\\Sigma)\\pi}{\\mathcal{N}(\\mu_0,\\Sigma)(1-\\pi)}.$$\n",
        "Let me compute the functional form of $f(x)$:\n",
        "\\begin{align}\n",
        "f(x) &\\equiv \\frac{\\mathcal{N}(\\mu_1,\\Sigma)\\pi}{\\mathcal{N}(\\mu_0,\\Sigma)(1-\\pi)},\\\\\n",
        "&= \\log\\mathcal{N}(\\mu_1,\\Sigma) -\\log\\mathcal{N}(\\mu_0,\\Sigma) +\\log\\frac{\\pi}{1-\\pi},\\\\\n",
        "&=-\\log2\\pi-\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x-\\mu_1)^T\\Sigma^{-1}(x-\\mu_1)-\\left(-\\log2\\pi -\\frac{1}{2}\\log|\\Sigma|-\\frac{1}{2}(x-\\mu_0)^T\\Sigma^{-1}(x-\\mu_0)\\right) +\\log\\frac{\\pi}{1-\\pi},\\\\\n",
        "&=\\frac{1}{2}\\left(-x^T\\Sigma^{-1}x+x^T\\Sigma^{-1}\\mu_1+\\mu_1^T\\Sigma^{-1}x -\\mu_1^T\\Sigma^{-1}\\mu_1+x^T\\Sigma^{-1}x-x^T\\Sigma^{-1}\\mu_0-\\mu_0^T\\Sigma^{-1}x+\\mu_0^T\\Sigma^{-1}\\mu_0\\right)+\\log\\frac{\\pi}{1-\\pi},\\\\\n",
        "&= \\mu_1^T\\Sigma^{-1}x - \\mu_0^T\\Sigma^{-1}x +\\frac{1}{2}(\\mu_0^T\\Sigma^{-1}\\mu_0-\\mu_1^T\\Sigma^{-1}\\mu_1)+\\log\\frac{\\pi}{1-\\pi},\\\\\n",
        "&=w^T\\tilde{x}.\n",
        "\\end{align}\n",
        "where I've defined\n",
        "$$w \\equiv \\begin{pmatrix}\n",
        "\\mu_1^T\\Sigma^{-1}-\\mu_0^T\\Sigma^{-1}\\\\\n",
        "\\mu_0^T\\Sigma^{-1}\\mu_0-\\mu_1^T\\Sigma^{-1}\\mu_1^T +\\log\\frac{\\pi}{1-\\pi}\n",
        "\\end{pmatrix};\\quad\\tilde{x}\\equiv\\begin{pmatrix}\n",
        "x\\\\\n",
        "1\n",
        "\\end{pmatrix}.$$\n",
        "This is very reminiscent of the logistic regression models. If the distribution is from the exponential family\n",
        "$$p(x|\\eta) = \\frac{h(x)}{\\exp(A(\\eta))}\\exp(\\eta^TT(x)),$$\n",
        "then logistic regression takes the form\n",
        "$$p(Y=1|x) = \\sigma(f(x)),$$\n",
        "with\n",
        "$$f(x)\\equiv w^T\\phi(x),\\quad w^T = \\begin{pmatrix}\n",
        "\\eta_1-\\eta_0\\\\\n",
        "A(\\eta_0)-A(\\eta_1) +\\log\\frac{\\pi}{1-\\pi}\n",
        "\\end{pmatrix},\\quad \\phi(x)\\equiv\\begin{pmatrix}\n",
        "T(x)\\\\\n",
        "1\n",
        "\\end{pmatrix}.$$\n",
        "So the functional form of $w^T$ in both models is different, and in logistic regression, the argument of the sigmoid function may be non-linear, as opposed to Fisher LDA which (the argument of the sigmoid) is linear in $x$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8YfRdrDxg21"
      },
      "source": [
        "**Question C:** Show that the decision function of QDA (with a probability threshold at $p(y=1\\mid x) = 0.5$) is the graph of a quadratic function. What happens when the covariance matrices are equal $\\Sigma_{0} = \\Sigma_{1}$ (connect it to the previous questions)?\n",
        "\n",
        "_Hint: The graph of a quadratic function is characterized by the implicit equation $x^{T}Ax + b^{T}x + c = 0$, with a matrix $A$, a vector $b$ and a scalar $c$._\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqmd8My_X6eQ"
      },
      "source": [
        "## Playground\n",
        "\n",
        "You are allowed to add as many cells and functions as you wish in this section, but not allowed to change the signature (name and inputs) of the functions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKiZeEJkLsfe"
      },
      "outputs": [],
      "source": [
        "def dummy_model(X, y):\n",
        "\n",
        "    \"\"\"\n",
        "    This is a dummy model to show you how your solution is expected to look like\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "\n",
        "        Returns:\n",
        "            dummy_classifier: dummy classification function\n",
        "    \"\"\"\n",
        "\n",
        "    # Just computing some means and standard deviations, nothing fancy\n",
        "    mu0, mu1 = X[:, 0].mean(), X[:, 1].mean()\n",
        "    s0, s1 = X[:, 0].std(), X[:, 1].std()\n",
        "\n",
        "\n",
        "    def dummy_classifier(x0, x1):\n",
        "        # Note how we use the variables mu0, mu1, s0 and s1 computed from the data\n",
        "        # to define this function. Their values get stored in the definition\n",
        "        # of the function itself, meaning that we don't have to store them explicitly.\n",
        "        return 0.5 * (1 + np.tanh((x0 - mu0)**2/s0 - (x1 - mu1)**2/s1))\n",
        "\n",
        "    return dummy_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wW5kP0b0Y0Sj"
      },
      "outputs": [],
      "source": [
        "def LDA(X, y):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of an LDA model\n",
        "\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "\n",
        "        Returns:\n",
        "            LDA_classifier: function taking two scalar inputs implementing the\n",
        "                            estimated p(y=1|x)\n",
        "    \"\"\"\n",
        "    # TODO: parameter estimation goes here\n",
        "\n",
        "    def LDA_classifier(x0, x1):\n",
        "         # TODO: use the variables estimated above to define p(y=1|(x0, x1))\n",
        "        return 0\n",
        "\n",
        "    return LDA_classifier\n",
        "\n",
        "\n",
        "def LogReg(X, y):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of a Logistic Regression model\n",
        "\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "\n",
        "        Returns:\n",
        "            LogReg_classifier: function taking two scalar inputs implementing the\n",
        "                               estimated p(y=1|x)\n",
        "    \"\"\"\n",
        "    # TODO: parameter estimation goes here\n",
        "\n",
        "    def LogReg_classifier(x0, x1):\n",
        "         # TODO: use the variables estimated above to define p(y=1|(x0, x1))\n",
        "        return 0\n",
        "\n",
        "    return LogReg_classifier\n",
        "\n",
        "\n",
        "def LinReg(X, y):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of a Linear Regression model\n",
        "\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "\n",
        "        Returns:\n",
        "            LinReg_classifier: function taking two scalar inputs implementing the\n",
        "                               estimated f(x)\n",
        "    \"\"\"\n",
        "    # TODO: parameter estimation goes here\n",
        "\n",
        "    def LinReg_classifier(x0, x1):\n",
        "         # TODO: use the variables estimated above to implement f(x0, x1)\n",
        "        return 0\n",
        "\n",
        "    return LinReg_classifier\n",
        "\n",
        "\n",
        "def QDA(X, y):\n",
        "    \"\"\"\n",
        "    Estimates the parameters of a QDA model\n",
        "\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "\n",
        "        Returns:\n",
        "            QDA_classifier: function taking two scalar inputs implementing the\n",
        "                            estimated p(y=1|x)\n",
        "    \"\"\"\n",
        "    # TODO: parameter estimation goes here\n",
        "\n",
        "    def QDA_classifier(x0, x1):\n",
        "         # TODO: use the variables estimated above to implement p(y=1|(x0, x1))\n",
        "        return 0\n",
        "\n",
        "    return QDA_classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls85ghIiHzb0"
      },
      "source": [
        "## Model Comparison\n",
        "\n",
        "You are allowed to change the cell contents unless explicitly noted otherwise. Use the provided cells to type your answers and fill the tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ9Oek1lpoRb"
      },
      "source": [
        "### Code for plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f30i-scwYYtA"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------- #\n",
        "#                      Do NOT modify this cell\n",
        "# ----------------------------------------------------------------------- #\n",
        "\n",
        "def pointwise_eval(f, umatrix, vmatrix):\n",
        "    \"\"\"\n",
        "    Evaluates the function f over a grid given by the matrices u and V\n",
        "\n",
        "        Inputs:\n",
        "            f: [callable] function (of two scalar inputs) to be evaluated\n",
        "            u: [n x m] matrix of values for the first input\n",
        "            v: [n x m] matrix of values for the second input\n",
        "\n",
        "        Returns:\n",
        "            z: [n x m] matrix of function values f(u_ij, v_ij) for i \\in [1..n], j \\in [1..m]\n",
        "    \"\"\"\n",
        "    n, m = umatrix.shape\n",
        "    z = np.zeros((n, m))\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "        z[i, j] = f(umatrix[i, j], vmatrix[i,j])\n",
        "    return z\n",
        "\n",
        "def plot_results(X, y, classifiers):\n",
        "    \"\"\"\n",
        "    Displays the behavior of the several classifiers via a 2x2 subplot.\n",
        "\n",
        "        Inputs:\n",
        "            X: [nx2] matrix of inputs\n",
        "            Y: [n] vector of labels\n",
        "            classifiers: [4] list of functions in EXACTLT the following order:\n",
        "                         [\"LDA\", \"LogReg\", \"LinReg\", \"QDA\"]\n",
        "\n",
        "        Output:\n",
        "            2x2 subplot with showing a scatter plot of the data, a contour plot\n",
        "            for each classifier function and the level set f(x)=0.5 in black.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(classifiers) == 4\n",
        "\n",
        "    labels = [\"LDA\", \"LogReg\", \"LinReg\", \"QDA\"]\n",
        "\n",
        "    # Find range in each dimension and setup a meshgrid\n",
        "    x0_list = np.linspace(X[:, 0].min(), X[:, 0].max(), 250)\n",
        "    x1_list = np.linspace(X[:, 1].min(), X[:, 1].max(), 250)\n",
        "    x0_v, x1_v = np.meshgrid(x0_list, x1_list, sparse=False, indexing='ij')\n",
        "\n",
        "    # Create 2x2 subplot\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(9, 9))\n",
        "    (ax1, ax2), (ax3, ax4) = axs\n",
        "\n",
        "    for ix, ax in enumerate([ax1, ax2, ax3, ax4]):\n",
        "\n",
        "        # Take an individual classifier\n",
        "        f = classifiers[ix]\n",
        "\n",
        "        if not f is None:\n",
        "\n",
        "            # Evaluate over the meshgrid and plot contour lines\n",
        "            # Z = f(x0_v, x1_v) # This is old code!!\n",
        "            Z = pointwise_eval(f, x0_v, x1_v)\n",
        "            contours = ax.contourf(x0_v, x1_v, Z, 5, cmap='RdBu', alpha=0.2);\n",
        "            ax.clabel(contours, inline=True, fontsize=10)\n",
        "            ax.contour(x0_v, x1_v, Z, [0.5], colors='black');\n",
        "\n",
        "            # Plot the labelled data on top\n",
        "            ax.scatter(X[y==0, 0], X[y==0, 1], marker=\"x\", c='r');\n",
        "            ax.scatter(X[y==1, 0], X[y==1, 1], marker=\"o\", facecolors='none', edgecolors='b');\n",
        "            ax.set_title(labels[ix])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ls9dSYYVQI"
      },
      "source": [
        "### Dataset A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIHOQVSzkYXQ"
      },
      "source": [
        "#### Parameter estimation and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ao9IpmNHryQ"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = data[\"train_A\"][:, 0:2], data[\"train_A\"][:, -1]\n",
        "X_test, y_test = data[\"test_A\"][:, 0:2], data[\"test_A\"][:, -1]\n",
        "\n",
        "\n",
        "# Note that by setting some of the entries to None you can implement and test each\n",
        "# of the methods one by one.\n",
        "\n",
        "# You can erase the following TWO lines once you are comfortable with the codebase.\n",
        "dummy_class_list = [dummy_model(X_train, y_train), None, None, None]\n",
        "plot_results(X_train, y_train, dummy_class_list)\n",
        "\n",
        "# Your classifiers list should always look like this. Run this when you\n",
        "# have implemented and tested the methods above.\n",
        "\n",
        "# classifiers = [model(X_train, y_train) for model in [LDA, LogReg, LinReg, QDA]]\n",
        "# plot_results(X_train, y_train, classifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A1J8geuj2Sk"
      },
      "source": [
        "#### Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdYgQ5AAll9y"
      },
      "source": [
        "Method | Train Acc. | Test Acc.\n",
        "--- | --- | ---\n",
        "*LDA* | ? % | ? %\n",
        "*LogReg* | ? % | ? %\n",
        "*LinReg* | ? % | ? %\n",
        "*QDA* | ? % | ? %\n",
        "\n",
        "\n",
        "**Question:** Is the misclassiffication error larger, smaller, or similar on the training and test data? Why?\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-dKJDJIlGNC"
      },
      "source": [
        "**Question:** Which methods yield very similar/dissimilar results? Which method yield the best results? Provide an interpretation.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fD576vob1Z"
      },
      "source": [
        "### Dataset B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyacdZmhob1c"
      },
      "source": [
        "#### Parameter estimation and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POjn4Xfzob1d"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = data[\"train_B\"][:, 0:2], data[\"train_B\"][:, -1]\n",
        "X_test, y_test = data[\"test_B\"][:, 0:2], data[\"test_B\"][:, -1]\n",
        "\n",
        "\n",
        "# Your classifiers list should always look like this. Run this when you\n",
        "# have implemented and tested the methods above.\n",
        "\n",
        "classifiers = [model(X_train, y_train) for model in [LDA, LogReg, LinReg, QDA]]\n",
        "plot_results(X_train, y_train, classifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2g7VdIkob1f"
      },
      "source": [
        "#### Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTPtPY5Lob1g"
      },
      "source": [
        "Method | Train Acc. | Test Acc.\n",
        "--- | --- | ---\n",
        "*LDA* | ? % | ? %\n",
        "*LogReg* | ? % | ? %\n",
        "*LinReg* | ? % | ? %\n",
        "*QDA* | ? % | ? %\n",
        "\n",
        "\n",
        "**Question:** Is the misclassiffication error larger, smaller, or similar on the training and test data? Why?\n",
        "\n",
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfujqpt2ob1h"
      },
      "source": [
        "**Question:** Which methods yield very similar/dissimilar results? Which method yield the best results? Provide an interpretation.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdQIo6j2olU7"
      },
      "source": [
        "### Dataset C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ0DVSy5olU9"
      },
      "source": [
        "#### Parameter estimation and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsj_8TKpolU-"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = data[\"train_C\"][:, 0:2], data[\"train_C\"][:, -1]\n",
        "X_test, y_test = data[\"test_C\"][:, 0:2], data[\"test_C\"][:, -1]\n",
        "\n",
        "\n",
        "# Your classifiers list should always look like this. Run this when you\n",
        "# have implemented and tested the methods above.\n",
        "\n",
        "classifiers = [model(X_train, y_train) for model in [LDA, LogReg, LinReg, QDA]]\n",
        "plot_results(X_train, y_train, classifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K89AXqq7olVC"
      },
      "source": [
        "#### Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcgvPEQ0olVD"
      },
      "source": [
        "Method | Train Acc. | Test Acc.\n",
        "--- | --- | ---\n",
        "*LDA* | ? % | ? %\n",
        "*LogReg* | ? % | ? %\n",
        "*LinReg* | ? % | ? %\n",
        "*QDA* | ? % | ? %\n",
        "\n",
        "\n",
        "**Question:** Is the misclassiffication error larger, smaller, or similar on the training and test data? Why?\n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_nlC87nolVD"
      },
      "source": [
        "**Question:** Which methods yield very similar/dissimilar results? Which method yield the best results? Provide an interpretation.\n",
        "\n",
        "**Answer:**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
